{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building Your Own Mini Agentic Framework\n",
        "\n",
        "This notebook breaks down how to create a lightweight agentic framework from scratch. We'll build each component step-by-step with clear explanations.\n",
        "\n",
        "## What is an Agentic Framework?\n",
        "\n",
        "An agentic framework provides:\n",
        "1. **Message handling** - Structured communication between user, agent, and tools\n",
        "2. **Tool system** - Functions the agent can call to interact with the world\n",
        "3. **Memory management** - Conversation history and context retention\n",
        "4. **Agent execution loop** - The core decision-making and execution cycle\n",
        "5. **Model abstraction** - Interface to different LLM providers\n",
        "\n",
        "Let's build each piece!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initial setup - run this first!\n",
        "# All necessary imports for the framework\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from typing import Optional, Dict, Any, List, Callable\n",
        "from datetime import datetime\n",
        "from abc import ABC, abstractmethod\n",
        "import inspect\n",
        "import json\n",
        "\n",
        "print(\"âœ… All imports loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Message Types\n",
        "\n",
        "The foundation of any agent framework is message passing. Messages represent the conversation between user, agent, and tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MessageRole(Enum):\n",
        "    \"\"\"Defines who sent the message in the conversation\"\"\"\n",
        "    USER = \"user\"\n",
        "    ASSISTANT = \"assistant\"\n",
        "    SYSTEM = \"system\"\n",
        "    TOOL = \"tool\"\n",
        "\n",
        "@dataclass\n",
        "class Message:\n",
        "    \"\"\"\n",
        "    Represents a single message in the conversation.\n",
        "    \n",
        "    Attributes:\n",
        "        role: Who sent the message (user, assistant, system, or tool)\n",
        "        content: The actual message text\n",
        "        name: Optional name for tool calls/results\n",
        "        tool_call_id: Links tool results back to tool calls\n",
        "        timestamp: When the message was created\n",
        "    \"\"\"\n",
        "    role: MessageRole\n",
        "    content: str\n",
        "    name: Optional[str] = None\n",
        "    tool_call_id: Optional[str] = None\n",
        "    timestamp: datetime = None\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        if self.timestamp is None:\n",
        "            self.timestamp = datetime.now()\n",
        "    \n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert message to dictionary format for LLM API calls\"\"\"\n",
        "        msg = {\"role\": self.role.value, \"content\": self.content}\n",
        "        if self.name:\n",
        "            msg[\"name\"] = self.name\n",
        "        if self.tool_call_id:\n",
        "            msg[\"tool_call_id\"] = self.tool_call_id\n",
        "        return msg\n",
        "\n",
        "# Example usage\n",
        "user_msg = Message(MessageRole.USER, \"What's the weather in New York?\")\n",
        "print(f\"User message: {user_msg.content}\")\n",
        "print(f\"Formatted: {user_msg.to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Tool System\n",
        "\n",
        "Tools are functions that agents can call to interact with external systems. Each tool needs:\n",
        "- A name and description (for the LLM to understand when to use it)\n",
        "- Parameters (what inputs it needs)\n",
        "- The actual function to execute\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Tool:\n",
        "    \"\"\"\n",
        "    Wraps a Python function as a tool the agent can use.\n",
        "    \n",
        "    The tool description and parameter info help the LLM decide:\n",
        "    1. When to call this tool\n",
        "    2. What arguments to pass\n",
        "    \n",
        "    Attributes:\n",
        "        name: Unique identifier for the tool\n",
        "        description: Human-readable explanation of what the tool does\n",
        "        func: The actual Python function to execute\n",
        "        parameters: JSON schema describing the function parameters\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    description: str\n",
        "    func: Callable\n",
        "    parameters: Dict[str, Any]\n",
        "    \n",
        "    @classmethod\n",
        "    def from_function(cls, func: Callable, name: str = None, description: str = None):\n",
        "        \"\"\"\n",
        "        Automatically create a Tool from a Python function.\n",
        "        Uses function signature and docstring to infer parameters.\n",
        "        \"\"\"\n",
        "        sig = inspect.signature(func)\n",
        "        name = name or func.__name__\n",
        "        description = description or func.__doc__ or \"\"\n",
        "        \n",
        "        # Extract parameter schema from function signature\n",
        "        properties = {}\n",
        "        required = []\n",
        "        \n",
        "        for param_name, param in sig.parameters.items():\n",
        "            if param_name == \"self\":\n",
        "                continue\n",
        "                \n",
        "            param_type = \"string\"  # default\n",
        "            if param.annotation != inspect.Parameter.empty:\n",
        "                if param.annotation == int:\n",
        "                    param_type = \"integer\"\n",
        "                elif param.annotation == float:\n",
        "                    param_type = \"number\"\n",
        "                elif param.annotation == bool:\n",
        "                    param_type = \"boolean\"\n",
        "                elif param.annotation == list:\n",
        "                    param_type = \"array\"\n",
        "            \n",
        "            properties[param_name] = {\"type\": param_type}\n",
        "            if param.default == inspect.Parameter.empty:\n",
        "                required.append(param_name)\n",
        "        \n",
        "        parameters = {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": properties,\n",
        "            \"required\": required\n",
        "        }\n",
        "        \n",
        "        return cls(name=name, description=description, func=func, parameters=parameters)\n",
        "    \n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert tool to OpenAI function calling format\"\"\"\n",
        "        return {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": self.name,\n",
        "                \"description\": self.description,\n",
        "                \"parameters\": self.parameters\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    async def call(self, **kwargs) -> Any:\n",
        "        \"\"\"Execute the tool with given arguments\"\"\"\n",
        "        if inspect.iscoroutinefunction(self.func):\n",
        "            return await self.func(**kwargs)\n",
        "        else:\n",
        "            return self.func(**kwargs)\n",
        "\n",
        "# Example: Create a simple weather tool\n",
        "def get_temperature(city: str) -> str:\n",
        "    \"\"\"Get the current temperature for a city\n",
        "    \n",
        "    Args:\n",
        "        city: The name of the city (e.g., \"New York\", \"London\")\n",
        "    \n",
        "    Returns:\n",
        "        Temperature string like \"22Â°C\"\n",
        "    \"\"\"\n",
        "    temperatures = {\n",
        "        \"New York\": \"22Â°C\",\n",
        "        \"London\": \"15Â°C\",\n",
        "        \"Tokyo\": \"18Â°C\",\n",
        "        \"Paris\": \"20Â°C\"\n",
        "    }\n",
        "    return temperatures.get(city, \"Unknown\")\n",
        "\n",
        "# Create tool from function\n",
        "weather_tool = Tool.from_function(get_temperature)\n",
        "print(f\"Tool name: {weather_tool.name}\")\n",
        "print(f\"Tool description: {weather_tool.description}\")\n",
        "print(f\"Tool schema:\\n{weather_tool.to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Memory Management\n",
        "\n",
        "Agents need to remember past conversations. We'll implement a simple memory system that:\n",
        "- Stores message history\n",
        "- Provides context for the LLM\n",
        "- Allows retrieval and filtering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Memory:\n",
        "    \"\"\"\n",
        "    Manages conversation history and context for the agent.\n",
        "    \n",
        "    This is a simple in-memory storage. In production, you might want:\n",
        "    - Persistent storage (database)\n",
        "    - Vector embeddings for semantic search\n",
        "    - Token limits and summarization\n",
        "    - Session management\n",
        "    \n",
        "    Attributes:\n",
        "        messages: List of all messages in the conversation\n",
        "        max_messages: Maximum number of messages to keep (None = unlimited)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, max_messages: Optional[int] = None):\n",
        "        self.messages: List[Message] = []\n",
        "        self.max_messages = max_messages\n",
        "    \n",
        "    def add_message(self, message: Message):\n",
        "        \"\"\"Add a message to memory\"\"\"\n",
        "        self.messages.append(message)\n",
        "        \n",
        "        # Trim old messages if we exceed max\n",
        "        if self.max_messages and len(self.messages) > self.max_messages:\n",
        "            # Keep system messages and recent messages\n",
        "            system_msgs = [m for m in self.messages if m.role == MessageRole.SYSTEM]\n",
        "            recent_msgs = self.messages[-self.max_messages:]\n",
        "            self.messages = system_msgs + recent_msgs[len(system_msgs):]\n",
        "    \n",
        "    def get_messages(self, include_system: bool = True) -> List[Message]:\n",
        "        \"\"\"\n",
        "        Get all messages, optionally filtering out system messages.\n",
        "        \n",
        "        System messages are usually kept in the conversation but\n",
        "        you might want to exclude them for some operations.\n",
        "        \"\"\"\n",
        "        if include_system:\n",
        "            return self.messages.copy()\n",
        "        return [m for m in self.messages if m.role != MessageRole.SYSTEM]\n",
        "    \n",
        "    def get_context_messages(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Get messages formatted for LLM API calls\"\"\"\n",
        "        return [msg.to_dict() for msg in self.messages]\n",
        "    \n",
        "    def clear(self):\n",
        "        \"\"\"Clear all messages except system messages\"\"\"\n",
        "        system_msgs = [m for m in self.messages if m.role == MessageRole.SYSTEM]\n",
        "        self.messages = system_msgs\n",
        "    \n",
        "    def get_recent(self, n: int) -> List[Message]:\n",
        "        \"\"\"Get the last N messages\"\"\"\n",
        "        return self.messages[-n:] if n > 0 else []\n",
        "\n",
        "# Example usage\n",
        "memory = Memory(max_messages=10)\n",
        "memory.add_message(Message(MessageRole.SYSTEM, \"You are a helpful assistant.\"))\n",
        "memory.add_message(Message(MessageRole.USER, \"Hello!\"))\n",
        "memory.add_message(Message(MessageRole.ASSISTANT, \"Hi there! How can I help you?\"))\n",
        "\n",
        "print(f\"Total messages: {len(memory.get_messages())}\")\n",
        "print(f\"Context for LLM: {len(memory.get_context_messages())} messages\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Model Abstraction\n",
        "\n",
        "We need a way to interact with different LLM providers (OpenAI, Ollama, etc.).\n",
        "This abstraction layer lets us swap models without changing the agent code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LLMProvider(ABC):\n",
        "    \"\"\"\n",
        "    Abstract base class for LLM providers.\n",
        "    \n",
        "    This defines the interface that all model providers must implement.\n",
        "    By abstracting this, we can support multiple providers:\n",
        "    - OpenAI\n",
        "    - Ollama\n",
        "    - Anthropic Claude\n",
        "    - Local models\n",
        "    - etc.\n",
        "    \"\"\"\n",
        "    \n",
        "    @abstractmethod\n",
        "    async def chat_completion(\n",
        "        self,\n",
        "        messages: List[Dict[str, Any]],\n",
        "        tools: Optional[List[Tool]] = None,\n",
        "        temperature: float = 0.7,\n",
        "        max_tokens: Optional[int] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate a chat completion from the LLM.\n",
        "        \n",
        "        Args:\n",
        "            messages: List of message dicts in OpenAI format\n",
        "            tools: Optional list of tools the model can use\n",
        "            temperature: Sampling temperature (0-2)\n",
        "            max_tokens: Maximum tokens to generate\n",
        "            \n",
        "        Returns:\n",
        "            Dict with 'content' and optional 'tool_calls'\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class OpenAIProvider(LLMProvider):\n",
        "    \"\"\"\n",
        "    OpenAI API provider implementation.\n",
        "    \n",
        "    Uses the OpenAI client to make chat completion requests.\n",
        "    Handles tool calling if tools are provided.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model: str = \"gpt-4o-mini\", api_key: Optional[str] = None, base_url: Optional[str] = None):\n",
        "        from openai import OpenAI\n",
        "        \n",
        "        self.model = model\n",
        "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
        "    \n",
        "    async def chat_completion(\n",
        "        self,\n",
        "        messages: List[Dict[str, Any]],\n",
        "        tools: Optional[List[Tool]] = None,\n",
        "        temperature: float = 0.7,\n",
        "        max_tokens: Optional[int] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Call OpenAI API\"\"\"\n",
        "        kwargs = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature,\n",
        "        }\n",
        "        \n",
        "        if max_tokens:\n",
        "            kwargs[\"max_tokens\"] = max_tokens\n",
        "        \n",
        "        if tools:\n",
        "            kwargs[\"tools\"] = [tool.to_dict() for tool in tools]\n",
        "            kwargs[\"tool_choice\"] = \"auto\"\n",
        "        \n",
        "        response = self.client.chat.completions.create(**kwargs)\n",
        "        \n",
        "        message = response.choices[0].message\n",
        "        result = {\"content\": message.content}\n",
        "        \n",
        "        # Extract tool calls if present\n",
        "        if message.tool_calls:\n",
        "            result[\"tool_calls\"] = []\n",
        "            for tool_call in message.tool_calls:\n",
        "                result[\"tool_calls\"].append({\n",
        "                    \"id\": tool_call.id,\n",
        "                    \"name\": tool_call.function.name,\n",
        "                    \"arguments\": json.loads(tool_call.function.arguments)\n",
        "                })\n",
        "        \n",
        "        return result\n",
        "\n",
        "\n",
        "class OllamaProvider(LLMProvider):\n",
        "    \"\"\"\n",
        "    Ollama provider implementation.\n",
        "    \n",
        "    Works with local Ollama models via OpenAI-compatible API.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model: str = \"llama3\", base_url: str = \"http://localhost:11434/v1\", api_key: str = \"ollama\"):\n",
        "        from openai import OpenAI\n",
        "        \n",
        "        self.model = model\n",
        "        self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
        "    \n",
        "    async def chat_completion(\n",
        "        self,\n",
        "        messages: List[Dict[str, Any]],\n",
        "        tools: Optional[List[Tool]] = None,\n",
        "        temperature: float = 0.7,\n",
        "        max_tokens: Optional[int] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Call Ollama via OpenAI-compatible API\"\"\"\n",
        "        # Note: Tool calling support may vary by Ollama model\n",
        "        kwargs = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature,\n",
        "        }\n",
        "        \n",
        "        if max_tokens:\n",
        "            kwargs[\"max_tokens\"] = max_tokens\n",
        "        \n",
        "        if tools:\n",
        "            kwargs[\"tools\"] = [tool.to_dict() for tool in tools]\n",
        "        \n",
        "        response = self.client.chat.completions.create(**kwargs)\n",
        "        \n",
        "        message = response.choices[0].message\n",
        "        result = {\"content\": message.content}\n",
        "        \n",
        "        # Extract tool calls if present\n",
        "        if hasattr(message, 'tool_calls') and message.tool_calls:\n",
        "            result[\"tool_calls\"] = []\n",
        "            for tool_call in message.tool_calls:\n",
        "                result[\"tool_calls\"].append({\n",
        "                    \"id\": tool_call.id,\n",
        "                    \"name\": tool_call.function.name,\n",
        "                    \"arguments\": json.loads(tool_call.function.arguments)\n",
        "                })\n",
        "        \n",
        "        return result\n",
        "\n",
        "print(\"Model providers defined!\")\n",
        "print(\"- OpenAIProvider: For OpenAI API\")\n",
        "print(\"- OllamaProvider: For local Ollama models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: The Agent - Core Execution Loop\n",
        "\n",
        "This is the heart of the framework. The agent:\n",
        "1. Receives user input\n",
        "2. Sends messages + tools to LLM\n",
        "3. If LLM wants to call a tool, execute it\n",
        "4. Send tool result back to LLM\n",
        "5. Return final response\n",
        "\n",
        "This is the \"ReAct\" pattern: Reasoning + Acting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    The main agent class that orchestrates the execution loop.\n",
        "    \n",
        "    This implements a simple ReAct (Reasoning + Acting) pattern:\n",
        "    1. User sends message\n",
        "    2. Agent thinks (LLM generates response)\n",
        "    3. If tool call needed, agent acts (executes tool)\n",
        "    4. Agent observes (gets tool result)\n",
        "    5. Agent thinks again with new information\n",
        "    6. Returns final response\n",
        "    \n",
        "    Attributes:\n",
        "        provider: The LLM provider to use\n",
        "        tools: List of available tools\n",
        "        memory: Conversation memory/history\n",
        "        system_prompt: Instructions for the agent\n",
        "        max_iterations: Max tool call iterations (prevents infinite loops)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        provider: LLMProvider,\n",
        "        tools: Optional[List[Tool]] = None,\n",
        "        memory: Optional[Memory] = None,\n",
        "        system_prompt: Optional[str] = None,\n",
        "        max_iterations: int = 10\n",
        "    ):\n",
        "        self.provider = provider\n",
        "        self.tools = {tool.name: tool for tool in (tools or [])}\n",
        "        self.memory = memory or Memory()\n",
        "        self.max_iterations = max_iterations\n",
        "        \n",
        "        # Add system prompt if provided\n",
        "        if system_prompt:\n",
        "            self.memory.add_message(Message(MessageRole.SYSTEM, system_prompt))\n",
        "    \n",
        "    async def run(self, user_input: str, reset_context: bool = False) -> str:\n",
        "        \"\"\"\n",
        "        Main execution method - processes user input and returns response.\n",
        "        \n",
        "        This implements the agent loop:\n",
        "        1. Add user message to memory\n",
        "        2. Get context messages\n",
        "        3. Call LLM with messages + tools\n",
        "        4. If tool calls, execute them and loop back\n",
        "        5. Return final response\n",
        "        \n",
        "        Args:\n",
        "            user_input: The user's message\n",
        "            reset_context: If True, clear conversation history first\n",
        "            \n",
        "        Returns:\n",
        "            Final response string from the agent\n",
        "        \"\"\"\n",
        "        if reset_context:\n",
        "            self.memory.clear()\n",
        "        \n",
        "        # Add user message\n",
        "        user_msg = Message(MessageRole.USER, user_input)\n",
        "        self.memory.add_message(user_msg)\n",
        "        \n",
        "        iteration = 0\n",
        "        while iteration < self.max_iterations:\n",
        "            iteration += 1\n",
        "            \n",
        "            # Get current context\n",
        "            messages = self.memory.get_context_messages()\n",
        "            \n",
        "            # Call LLM\n",
        "            response = await self.provider.chat_completion(\n",
        "                messages=messages,\n",
        "                tools=list(self.tools.values()) if self.tools else None\n",
        "            )\n",
        "            \n",
        "            # Handle tool calls\n",
        "            if response.get(\"tool_calls\"):\n",
        "                # Add assistant message with tool calls to memory\n",
        "                assistant_msg = Message(MessageRole.ASSISTANT, response.get(\"content\", \"\"))\n",
        "                self.memory.add_message(assistant_msg)\n",
        "                \n",
        "                # Execute each tool call\n",
        "                for tool_call in response[\"tool_calls\"]:\n",
        "                    tool_id = tool_call[\"id\"]\n",
        "                    tool_name = tool_call[\"name\"]\n",
        "                    tool_args = tool_call[\"arguments\"]\n",
        "                    \n",
        "                    # Find and execute tool\n",
        "                    if tool_name in self.tools:\n",
        "                        tool = self.tools[tool_name]\n",
        "                        try:\n",
        "                            tool_result = await tool.call(**tool_args)\n",
        "                            result_str = str(tool_result) if not isinstance(tool_result, str) else tool_result\n",
        "                        except Exception as e:\n",
        "                            result_str = f\"Error: {str(e)}\"\n",
        "                    else:\n",
        "                        result_str = f\"Tool '{tool_name}' not found\"\n",
        "                    \n",
        "                    # Add tool result to memory\n",
        "                    tool_msg = Message(\n",
        "                        MessageRole.TOOL,\n",
        "                        result_str,\n",
        "                        name=tool_name,\n",
        "                        tool_call_id=tool_id\n",
        "                    )\n",
        "                    self.memory.add_message(tool_msg)\n",
        "                \n",
        "                # Continue loop to let LLM process tool results\n",
        "                continue\n",
        "            \n",
        "            # No tool calls - we have the final answer\n",
        "            content = response.get(\"content\", \"\")\n",
        "            if content:\n",
        "                assistant_msg = Message(MessageRole.ASSISTANT, content)\n",
        "                self.memory.add_message(assistant_msg)\n",
        "                return content\n",
        "        \n",
        "        # Max iterations reached\n",
        "        return \"Error: Maximum iterations reached. The agent may be stuck in a loop.\"\n",
        "\n",
        "print(\"Agent class defined!\")\n",
        "print(\"The agent handles the complete execution loop automatically.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Putting It All Together - Example Usage\n",
        "\n",
        "Let's create a working example that demonstrates the full framework!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Example 1: Create tools\n",
        "def get_temperature(city: str) -> str:\n",
        "    \"\"\"Get the current temperature for a city\"\"\"\n",
        "    temperatures = {\n",
        "        \"New York\": \"22Â°C\",\n",
        "        \"London\": \"15Â°C\",\n",
        "        \"Tokyo\": \"18Â°C\",\n",
        "        \"Paris\": \"20Â°C\"\n",
        "    }\n",
        "    return temperatures.get(city, \"Unknown\")\n",
        "\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"Evaluate a mathematical expression\n",
        "    \n",
        "    Args:\n",
        "        expression: A math expression like \"2 + 2\" or \"10 * 5\"\n",
        "    \n",
        "    Returns:\n",
        "        The result as a string\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Safety: Only allow basic math operations\n",
        "        allowed_chars = set(\"0123456789+-*/(). \")\n",
        "        if not all(c in allowed_chars for c in expression):\n",
        "            return \"Error: Invalid characters in expression\"\n",
        "        result = eval(expression)\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Create tools\n",
        "tools = [\n",
        "    Tool.from_function(get_temperature),\n",
        "    Tool.from_function(calculate, description=\"Performs basic math calculations\")\n",
        "]\n",
        "\n",
        "print(\"Tools created:\")\n",
        "for tool in tools:\n",
        "    print(f\"  - {tool.name}: {tool.description}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Create agent with OpenAI\n",
        "# Uncomment to use OpenAI (requires API key in .env)\n",
        "\n",
        "# provider = OpenAIProvider(\n",
        "#     model=\"gpt-4o-mini\",\n",
        "#     api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        "# )\n",
        "\n",
        "# agent = Agent(\n",
        "#     provider=provider,\n",
        "#     tools=tools,\n",
        "#     system_prompt=\"You are a helpful assistant. Use tools when appropriate.\",\n",
        "#     max_iterations=5\n",
        "# )\n",
        "\n",
        "# # Test the agent\n",
        "# result = await agent.run(\"What's the temperature in New York and what's 15 * 8?\")\n",
        "# print(f\"Agent response: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Create agent with Ollama (if you have it running locally)\n",
        "# Adjust the base_url to match your Ollama instance\n",
        "\n",
        "# provider = OllamaProvider(\n",
        "#     model=\"llama3\",\n",
        "#     base_url=\"http://localhost:11434/v1\"\n",
        "# )\n",
        "\n",
        "# agent = Agent(\n",
        "#     provider=provider,\n",
        "#     tools=tools,\n",
        "#     system_prompt=\"You are a helpful assistant. Use tools when appropriate.\",\n",
        "#     max_iterations=5\n",
        "# )\n",
        "\n",
        "# # Test the agent\n",
        "# result = await agent.run(\"What's the temperature in Tokyo?\")\n",
        "# print(f\"Agent response: {result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Advanced Features - What to Add Next\n",
        "\n",
        "Here are some enhancements you might want to add:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Streaming Responses\n",
        "# Instead of waiting for complete response, stream tokens as they're generated\n",
        "\n",
        "class StreamingAgent(Agent):\n",
        "    \"\"\"Agent that supports streaming responses\"\"\"\n",
        "    \n",
        "    async def run_stream(self, user_input: str):\n",
        "        \"\"\"Generator that yields tokens as they're generated\"\"\"\n",
        "        user_msg = Message(MessageRole.USER, user_input)\n",
        "        self.memory.add_message(user_msg)\n",
        "        \n",
        "        # Implementation would use provider's streaming API\n",
        "        # This is a placeholder showing the pattern\n",
        "        messages = self.memory.get_context_messages()\n",
        "        \n",
        "        # In real implementation, you'd stream from provider:\n",
        "        # async for chunk in provider.chat_completion_stream(...):\n",
        "        #     yield chunk\n",
        "        pass\n",
        "\n",
        "# 2. Function Calling with Pydantic Models\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class WeatherRequest(BaseModel):\n",
        "    city: str\n",
        "    unit: str = \"celsius\"\n",
        "\n",
        "# Tools can use Pydantic for validation:\n",
        "# tool = Tool.from_pydantic(WeatherRequest, func=get_weather)\n",
        "\n",
        "# 3. Memory with Vector Search\n",
        "# Store messages with embeddings, search by semantic similarity\n",
        "class VectorMemory(Memory):\n",
        "    \"\"\"Memory with vector search capabilities\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embeddings = []  # Would store vector embeddings\n",
        "    \n",
        "    def search_similar(self, query: str, limit: int = 5):\n",
        "        \"\"\"Find similar past conversations\"\"\"\n",
        "        # Implementation would:\n",
        "        # 1. Embed the query\n",
        "        # 2. Find similar message embeddings\n",
        "        # 3. Return relevant context\n",
        "        pass\n",
        "\n",
        "# 4. Multi-Agent Systems\n",
        "class MultiAgentSystem:\n",
        "    \"\"\"Orchestrates multiple agents working together\"\"\"\n",
        "    \n",
        "    def __init__(self, agents: Dict[str, Agent]):\n",
        "        self.agents = agents\n",
        "    \n",
        "    async def route(self, user_input: str) -> str:\n",
        "        \"\"\"Route user input to appropriate agent\"\"\"\n",
        "        # Could use a router agent to decide which specialist to use\n",
        "        # Or implement rule-based routing\n",
        "        pass\n",
        "\n",
        "# 5. Observability & Logging\n",
        "class ObservableAgent(Agent):\n",
        "    \"\"\"Agent with built-in logging and metrics\"\"\"\n",
        "    \n",
        "    async def run(self, user_input: str, **kwargs):\n",
        "        # Log input\n",
        "        print(f\"[INPUT] {user_input}\")\n",
        "        \n",
        "        # Track metrics\n",
        "        start_time = datetime.now()\n",
        "        result = await super().run(user_input, **kwargs)\n",
        "        duration = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        # Log output\n",
        "        print(f\"[OUTPUT] {result}\")\n",
        "        print(f\"[METRICS] Duration: {duration}s, Messages: {len(self.memory.messages)}\")\n",
        "        \n",
        "        return result\n",
        "\n",
        "print(\"Advanced patterns shown above:\")\n",
        "print(\"1. Streaming responses\")\n",
        "print(\"2. Pydantic model validation\")\n",
        "print(\"3. Vector memory search\")\n",
        "print(\"4. Multi-agent orchestration\")\n",
        "print(\"5. Observability & logging\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Framework Architecture Summary\n",
        "\n",
        "Here's how all the pieces fit together:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                     User Input                          â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                    â”‚\n",
        "                    â–¼\n",
        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "        â”‚      Agent.run()      â”‚\n",
        "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                    â”‚\n",
        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "        â”‚   Memory.add_message  â”‚  â† Stores conversation\n",
        "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                    â”‚\n",
        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "        â”‚  Get context messages â”‚  â† Retrieves history\n",
        "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                    â”‚\n",
        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "        â”‚  LLMProvider.chat()   â”‚  â† Calls LLM API\n",
        "        â”‚  + Tools schema        â”‚\n",
        "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                    â”‚\n",
        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "        â”‚   Tool calls?         â”‚\n",
        "        â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜\n",
        "            â”‚ Yes           â”‚ No\n",
        "            â–¼               â–¼\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚ Execute Tool  â”‚   â”‚ Return Final â”‚\n",
        "    â”‚ Store Result  â”‚   â”‚   Response   â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "            â”‚\n",
        "            â””â”€â”€â”€â–º Loop back to LLM\n",
        "```\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **Message**: Data structure for conversation\n",
        "2. **Tool**: Wrapper for functions agent can call\n",
        "3. **Memory**: Manages conversation history\n",
        "4. **LLMProvider**: Abstracts model API calls\n",
        "5. **Agent**: Orchestrates the execution loop\n",
        "\n",
        "### The Loop:\n",
        "\n",
        "1. User sends message â†’ stored in memory\n",
        "2. Agent gets context â†’ sends to LLM\n",
        "3. LLM responds â†’ might call tools\n",
        "4. Tools execute â†’ results stored\n",
        "5. Loop back to step 2 with tool results\n",
        "6. LLM gives final answer â†’ return to user\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Comparison with Existing Frameworks\n",
        "\n",
        "### How this compares to:\n",
        "\n",
        "**Pydantic AI** (which you've been using):\n",
        "- âœ… Similar concepts (Agent, Tools, Messages)\n",
        "- âœ… This is more minimal and educational\n",
        "- âœ… Pydantic AI has more features (validation, streaming, etc.)\n",
        "\n",
        "**LangChain**:\n",
        "- âœ… LangChain is much larger with many integrations\n",
        "- âœ… This framework is ~200 lines vs LangChain's thousands\n",
        "- âœ… Good for learning, LangChain for production\n",
        "\n",
        "**AutoGen/CrewAI**:\n",
        "- âœ… Those focus on multi-agent coordination\n",
        "- âœ… This is a single-agent framework\n",
        "- âœ… Can extend this to multi-agent later\n",
        "\n",
        "### When to use this framework:\n",
        "- ğŸ“ Learning how agents work internally\n",
        "- ğŸš€ Building custom agentic workflows\n",
        "- ğŸ› ï¸ Simple use cases where full frameworks are overkill\n",
        "- ğŸ§ª Prototyping before committing to a framework\n",
        "\n",
        "### When to use existing frameworks:\n",
        "- ğŸ“¦ Production applications (use Pydantic AI, LangChain)\n",
        "- ğŸŒ Need many integrations (use LangChain)\n",
        "- ğŸ‘¥ Multi-agent systems (use AutoGen, CrewAI)\n",
        "- ğŸ”§ Complex tool orchestration (use specialized frameworks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: Next Steps & Extensions\n",
        "\n",
        "### Ideas for extending this framework:\n",
        "\n",
        "1. **Persistent Memory**: Save conversations to a database\n",
        "2. **Tool Caching**: Cache tool results to avoid redundant calls\n",
        "3. **Error Handling**: Better error recovery and retry logic\n",
        "4. **Cost Tracking**: Monitor token usage and costs\n",
        "5. **Rate Limiting**: Prevent too many API calls\n",
        "6. **Structured Output**: Use Pydantic models for agent responses\n",
        "7. **Planner Agents**: Add planning/reflection capabilities\n",
        "8. **Workflow Orchestration**: Chain multiple agents together\n",
        "9. **Observability**: Add tracing, logging, metrics\n",
        "10. **Configuration**: YAML/JSON config files for agents\n",
        "\n",
        "### Try building:\n",
        "- A research agent that searches the web\n",
        "- A coding assistant that can read/write files\n",
        "- A data analysis agent that queries databases\n",
        "- A customer service agent with knowledge base\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
